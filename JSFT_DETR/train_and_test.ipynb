{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "472a18aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images: 1195\n",
      "Train images (80%): 956\n",
      "Val images (20%): 239\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'MaskDataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 50\u001b[39m\n\u001b[32m     44\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mVal images (20%): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(val_img_paths)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     47\u001b[39m \u001b[38;5;66;03m# -----------------------------\u001b[39;00m\n\u001b[32m     48\u001b[39m \u001b[38;5;66;03m# 4. Train Dataset (8Îßå ÏÇ¨Ïö©)\u001b[39;00m\n\u001b[32m     49\u001b[39m \u001b[38;5;66;03m# -----------------------------\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m train_dataset = MaskDataset(\n\u001b[32m     51\u001b[39m     img_paths=train_img_paths,\n\u001b[32m     52\u001b[39m     mask_dir=mask_dir,\n\u001b[32m     53\u001b[39m     img_size=\u001b[32m640\u001b[39m\n\u001b[32m     54\u001b[39m )\n\u001b[32m     56\u001b[39m train_loader = DataLoader(\n\u001b[32m     57\u001b[39m     train_dataset,\n\u001b[32m     58\u001b[39m     batch_size=\u001b[32m1\u001b[39m,  \u001b[38;5;66;03m# 2 ‚Üí 1 (Î©îÎ™®Î¶¨ Ï†àÏïΩ)\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     61\u001b[39m     collate_fn=collate_fn\n\u001b[32m     62\u001b[39m )\n",
      "\u001b[31mNameError\u001b[39m: name 'MaskDataset' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.ops import box_iou\n",
    "\n",
    "from MaskDataset import MaskDataset\n",
    "from SEPN import ResNet18Backbone, SEPN\n",
    "from DY_PS import DySample, PSConv\n",
    "from hybrid_encoder import HybridEncoderBlock\n",
    "from Hungrian_match import HungarianMatcher, RTDETRDetection, SetCriterion\n",
    "\n",
    "# -----------------------------\n",
    "# 1. collate_fn (?? ???)\n",
    "# -----------------------------\n",
    "def collate_fn(batch):\n",
    "    imgs = torch.stack([b[0] for b in batch], dim=0)\n",
    "    targets = [b[1] for b in batch]\n",
    "    return imgs, targets\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 2. ??? ?? ??\n",
    "# -----------------------------\n",
    "def get_image_paths(img_dir, exts=(\".jpg\", \".png\", \".jpeg\", \".bmp\")):\n",
    "    return [\n",
    "        os.path.join(img_dir, f)\n",
    "        for f in os.listdir(img_dir)\n",
    "        if f.lower().endswith(exts)\n",
    "    ]\n",
    "\n",
    "\n",
    "img_dir = \"/data2/project/2025summer/yjh0913/DB-1/Images\"      # ? ??? ??\n",
    "mask_dir = \"/data2/project/2025summer/yjh0913/DB-1/Masks\"      # ? ??? ??\n",
    "\n",
    "all_img_paths = get_image_paths(img_dir)\n",
    "print(f\"Total images: {len(all_img_paths)}\")\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 3. 8:2 ?? (??? train?? 8? ??)\n",
    "# -----------------------------\n",
    "random.seed(42)\n",
    "random.shuffle(all_img_paths)\n",
    "\n",
    "split_idx = int(len(all_img_paths) * 0.8)\n",
    "train_img_paths = all_img_paths[:split_idx]\n",
    "val_img_paths   = all_img_paths[split_idx:]   # ? ?????? train?? ? ?\n",
    "\n",
    "print(f\"Train images (80%): {len(train_img_paths)}\")\n",
    "print(f\"Val images (20%): {len(val_img_paths)}\")\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 4. Train Dataset (8? ??)\n",
    "# -----------------------------\n",
    "train_dataset = MaskDataset(\n",
    "    img_paths=train_img_paths,\n",
    "    mask_dir=mask_dir,\n",
    "    img_size=640\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=1,  # 2 ? 1 (??? ??)\n",
    "    shuffle=True,\n",
    "    num_workers=0,  # ?????? ????\n",
    "    collate_fn=collate_fn\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28908f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Hungrian_match import RTDETRDetection\n",
    "\n",
    "class JFSTDETR(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_classes=1,\n",
    "        hidden_dim=256,\n",
    "        num_queries=100\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # Backbone\n",
    "        self.backbone = ResNet18Backbone()\n",
    "\n",
    "        # SEPN\n",
    "        self.sepn = SEPN()\n",
    "\n",
    "        # Channel alignment\n",
    "        self.proj3 = nn.Conv2d(128, hidden_dim, 1)\n",
    "        self.proj4 = nn.Conv2d(256, hidden_dim, 1)\n",
    "        self.proj5 = nn.Conv2d(512, hidden_dim, 1)\n",
    "\n",
    "        # DySample + PSConv\n",
    "        self.refine = DySample(hidden_dim)\n",
    "        \n",
    "        # Channel restore after DySample (pixel_shuffle reduces channels)\n",
    "        self.restore3 = nn.Conv2d(hidden_dim // 4, hidden_dim, 1)\n",
    "        self.restore4 = nn.Conv2d(hidden_dim // 4, hidden_dim, 1)\n",
    "        self.restore5 = nn.Conv2d(hidden_dim // 4, hidden_dim, 1)\n",
    "\n",
    "        # Hybrid Encoder\n",
    "        self.encoder = HybridEncoderBlock(hidden_dim)\n",
    "\n",
    "        # Detection head\n",
    "        self.detector = RTDETRDetection(\n",
    "            hidden_dim=hidden_dim,\n",
    "            num_classes=num_classes,\n",
    "            num_queries=num_queries\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Backbone\n",
    "        p2, p3, p4, p5 = self.backbone(x)\n",
    "\n",
    "        # SEPN\n",
    "        p3, p4, p5 = self.sepn(p2, p3, p4, p5)\n",
    "\n",
    "        # Channel align\n",
    "        p3 = self.proj3(p3)\n",
    "        p4 = self.proj4(p4)\n",
    "        p5 = self.proj5(p5)\n",
    "\n",
    "        # DySample + restore channels\n",
    "        p3 = self.refine(p3)\n",
    "        p3 = self.restore3(p3)\n",
    "        \n",
    "        p4 = self.refine(p4)\n",
    "        p4 = self.restore4(p4)\n",
    "        \n",
    "        p5 = self.refine(p5)\n",
    "        p5 = self.restore5(p5)\n",
    "\n",
    "        # Hybrid Encoder\n",
    "        p3, p4, p5 = self.encoder(p3, p4, p5)\n",
    "\n",
    "        # Flatten (RT-DETR Î∞©Ïãù)\n",
    "        B, C, H3, W3 = p3.shape\n",
    "        mem3 = p3.flatten(2).permute(0, 2, 1)\n",
    "        mem4 = p4.flatten(2).permute(0, 2, 1)\n",
    "        mem5 = p5.flatten(2).permute(0, 2, 1)\n",
    "        memory = torch.cat([mem3, mem4, mem5], dim=1)\n",
    "\n",
    "        # Detection\n",
    "        return self.detector(memory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841cf0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = JFSTDETR(\n",
    "    num_classes=1,\n",
    "    hidden_dim=128,  # 256 ‚Üí 128 (Î©îÎ™®Î¶¨ Ï†àÏïΩ)\n",
    "    num_queries=50   # 100 ‚Üí 50 (Î©îÎ™®Î¶¨ Ï†àÏïΩ)\n",
    ").to(device)\n",
    "\n",
    "matcher = HungarianMatcher(\n",
    "    cost_class=1,\n",
    "    cost_bbox=5,\n",
    "    cost_giou=2\n",
    ")\n",
    "\n",
    "criterion = SetCriterion(\n",
    "    num_classes=1,\n",
    "    matcher=matcher,\n",
    "    weight_dict={\n",
    "        \"loss_ce\": 2,\n",
    "        \"loss_bbox\": 5,\n",
    "        \"loss_giou\": 2\n",
    "    }\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=1e-5,  # ÌïôÏäµÎ•† ÎÇÆÏ∂§ (1e-4 ‚Üí 1e-5)\n",
    "    weight_decay=1e-4\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd20d1a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model / DataLoader / Criterion / Optimizer confirmed\n",
      "[Epoch 000] Loss: 10.4443 | CE: 0.693 | BBox: 0.887 | GIoU: 1.936 | Skipped batches: 0 | NaN batches: 0\n",
      "‚úÖ Saved best model (loss=10.4443)\n",
      "[Epoch 001] Loss: 10.4665 | CE: 0.693 | BBox: 1.005 | GIoU: 1.980 | Skipped batches: 0 | NaN batches: 0\n",
      "[Epoch 002] Loss: 10.4735 | CE: 0.693 | BBox: 0.971 | GIoU: 1.947 | Skipped batches: 0 | NaN batches: 0\n",
      "[Epoch 003] Loss: 10.4453 | CE: 0.693 | BBox: 1.143 | GIoU: 1.966 | Skipped batches: 0 | NaN batches: 0\n",
      "[Epoch 004] Loss: 10.4507 | CE: 0.693 | BBox: 0.767 | GIoU: 1.985 | Skipped batches: 0 | NaN batches: 0\n",
      "[Epoch 005] Loss: 10.4531 | CE: 0.693 | BBox: 1.179 | GIoU: 1.860 | Skipped batches: 0 | NaN batches: 0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "save_dir = \"./checkpoints\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "best_loss = float(\"inf\")\n",
    "num_epochs = 100\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for imgs, targets in train_loader:\n",
    "        imgs = imgs.to(device)\n",
    "        targets = [\n",
    "            {\n",
    "                \"boxes\": t[\"boxes\"].to(device),\n",
    "                \"labels\": t[\"labels\"].to(device)\n",
    "            }\n",
    "            for t in targets\n",
    "        ]\n",
    "\n",
    "        outputs = model(imgs)\n",
    "        loss, loss_dict = criterion(outputs, targets)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "\n",
    "    print(\n",
    "        f\"[Epoch {epoch:03d}] \"\n",
    "        f\"Loss: {avg_loss:.4f} | \"\n",
    "        f\"CE: {loss_dict['loss_ce']:.3f}, \"\n",
    "        f\"BBox: {loss_dict['loss_bbox']:.3f}, \"\n",
    "        f\"GIoU: {loss_dict['loss_giou']:.3f}\"\n",
    "    )\n",
    "\n",
    "    # üî• best weight Ï†ÄÏû•\n",
    "    if avg_loss < best_loss:\n",
    "        best_loss = avg_loss\n",
    "        torch.save(\n",
    "            {\n",
    "                \"epoch\": epoch,\n",
    "                \"model_state_dict\": model.state_dict(),\n",
    "                \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                \"loss\": best_loss\n",
    "            },\n",
    "            os.path.join(save_dir, \"jfst_detr_best.pth\")\n",
    "        )\n",
    "        print(f\"‚úÖ Saved best model (loss={best_loss:.4f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18b5a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torchvision.ops import box_iou\n",
    "\n",
    "# ===============================\n",
    "# box utils\n",
    "# ===============================\n",
    "def cxcywh_to_xyxy(boxes):\n",
    "    cx, cy, w, h = boxes.unbind(-1)\n",
    "    return torch.stack([\n",
    "        cx - w / 2,\n",
    "        cy - h / 2,\n",
    "        cx + w / 2,\n",
    "        cy + h / 2\n",
    "    ], dim=-1)\n",
    "\n",
    "# ===============================\n",
    "# postprocess (DETR style)\n",
    "# ===============================\n",
    "def postprocess(pred_logits, pred_boxes, score_thresh=0.5):\n",
    "    probs = F.softmax(pred_logits, dim=-1)\n",
    "    scores, _ = probs[..., :-1].max(dim=-1)  # remove background\n",
    "    keep = scores > score_thresh\n",
    "    return pred_boxes[keep], scores[keep]\n",
    "\n",
    "# ===============================\n",
    "# TP / FP / FN\n",
    "# ===============================\n",
    "def calc_tp_fp_fn(pred_boxes, gt_boxes, iou_thresh=0.5):\n",
    "    if len(pred_boxes) == 0:\n",
    "        return 0, 0, len(gt_boxes)\n",
    "\n",
    "    ious = box_iou(pred_boxes, gt_boxes)\n",
    "    tp = 0\n",
    "    matched = set()\n",
    "\n",
    "    for i in range(len(pred_boxes)):\n",
    "        max_iou, idx = ious[i].max(dim=0)\n",
    "        if max_iou >= iou_thresh and idx.item() not in matched:\n",
    "            tp += 1\n",
    "            matched.add(idx.item())\n",
    "\n",
    "    fp = len(pred_boxes) - tp\n",
    "    fn = len(gt_boxes) - tp\n",
    "    return tp, fp, fn\n",
    "\n",
    "# ===============================\n",
    "# AP (mAP@0.5)\n",
    "# ===============================\n",
    "def compute_ap(recalls, precisions):\n",
    "    recalls = np.concatenate(([0.], recalls, [1.]))\n",
    "    precisions = np.concatenate(([0.], precisions, [0.]))\n",
    "\n",
    "    for i in range(len(precisions) - 1, 0, -1):\n",
    "        precisions[i - 1] = max(precisions[i - 1], precisions[i])\n",
    "\n",
    "    idx = np.where(recalls[1:] != recalls[:-1])[0]\n",
    "    return np.sum((recalls[idx + 1] - recalls[idx]) * precisions[idx + 1])\n",
    "\n",
    "# ===============================\n",
    "# evaluation\n",
    "# ===============================\n",
    "@torch.no_grad()\n",
    "@torch.no_grad()\n",
    "def evaluate(model, dataloader, device, score_thresh=0.5, iou_thresh=0.5):\n",
    "    model.eval()\n",
    "\n",
    "    total_tp = total_fp = total_fn = 0\n",
    "    precisions, recalls = [], []\n",
    "\n",
    "    for imgs, targets in dataloader:\n",
    "        imgs = imgs.to(device)\n",
    "\n",
    "        # üî• FP16 + no grad\n",
    "        with torch.cuda.amp.autocast():\n",
    "            outputs = model(imgs)\n",
    "\n",
    "        # üî• ÌïÑÏöîÌïú Í≤ÉÎßå CPUÎ°ú Ï¶âÏãú Ïù¥Îèô\n",
    "        pred_logits = outputs[\"pred_logits\"].cpu()\n",
    "        pred_boxes  = outputs[\"pred_boxes\"].cpu()\n",
    "\n",
    "        # üî• GPU Î©îÎ™®Î¶¨ Ï¶âÏãú Ìï¥Ï†ú\n",
    "        del outputs, imgs\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        for i in range(len(pred_logits)):\n",
    "            pb, _ = postprocess(\n",
    "                pred_logits[i],\n",
    "                pred_boxes[i],\n",
    "                score_thresh\n",
    "            )\n",
    "\n",
    "            gt = targets[i][\"boxes\"]\n",
    "\n",
    "            pb = cxcywh_to_xyxy(pb)\n",
    "            gt = cxcywh_to_xyxy(gt)\n",
    "\n",
    "            tp, fp, fn = calc_tp_fp_fn(pb, gt, iou_thresh)\n",
    "\n",
    "            total_tp += tp\n",
    "            total_fp += fp\n",
    "            total_fn += fn\n",
    "\n",
    "            p = tp / (tp + fp + 1e-6)\n",
    "            r = tp / (tp + fn + 1e-6)\n",
    "            precisions.append(p)\n",
    "            recalls.append(r)\n",
    "\n",
    "        # üî• CPU ÌÖêÏÑúÎèÑ Ï†ïÎ¶¨\n",
    "        del pred_logits, pred_boxes\n",
    "\n",
    "    precision = total_tp / (total_tp + total_fp + 1e-6)\n",
    "    recall    = total_tp / (total_tp + total_fn + 1e-6)\n",
    "    f1        = 2 * precision * recall / (precision + recall + 1e-6)\n",
    "    map50     = compute_ap(np.array(recalls), np.array(precisions))\n",
    "\n",
    "    return precision, recall, f1, map50\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# RUN (load weight ‚Üí test 20%)\n",
    "# ===============================\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = JFSTDETR(\n",
    "    num_classes=1,\n",
    "    hidden_dim=128,  # 256 ‚Üí 128\n",
    "    num_queries=50   # 100 ‚Üí 50\n",
    ").to(device)\n",
    "\n",
    "ckpt = torch.load(\"./checkpoints/jfst_detr_best.pth\", map_location=device)\n",
    "model.load_state_dict(ckpt[\"model_state_dict\"])\n",
    "\n",
    "# val_img_pathsÏóêÏÑú DataLoader ÏÉùÏÑ±\n",
    "val_dataset = MaskDataset(\n",
    "    img_paths=val_img_paths,\n",
    "    mask_dir=mask_dir,\n",
    "    img_size=640\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=1,  # 2 ‚Üí 1 (Î©îÎ™®Î¶¨ Ï†àÏïΩ)\n",
    "    shuffle=False,\n",
    "    num_workers=0,  # Î©ÄÌã∞ÌîÑÎ°úÏÑ∏Ïã± ÎπÑÌôúÏÑ±Ìôî\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "precision, recall, f1, map50 = evaluate(\n",
    "    model,\n",
    "    val_loader,  # DataLoader Ï†ÑÎã¨\n",
    "    device,\n",
    "    score_thresh=0.5,\n",
    "    iou_thresh=0.5\n",
    ")\n",
    "\n",
    "print(f\"Precision : {precision:.4f}\")\n",
    "print(f\"Recall    : {recall:.4f}\")\n",
    "print(f\"F1-score  : {f1:.4f}\")\n",
    "print(f\"mAP@0.5   : {map50:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b446b50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yjh",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}